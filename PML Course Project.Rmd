---
title: "Practical Machine Learning"
author: "Davis Clark"
date: "Sunday, July 26, 2015"
output: html_document
---
```{r, echo=FALSE, message=FALSE}
library(caret)
library(rpart)
library(randomForest)
```
As always, before one sets out to build anything, one must gather the required materials and prepare their workspace:

We read in the datasets we seek to analyze: 
```{r}
testing <- read.csv("data/testing.csv")
training <- read.csv("data/training.csv")
```
Set the seed for reproducible results: 
```{r partition}
### Set seed to day I was born
set.seed(5251990)
```
And partition the training dataset to create a subset for preliminary testing:
```{r}
### Create test set to validate algorithm before official application
intrain<-createDataPartition(y=training$classe,p=0.7,list=FALSE)
train<-training[intrain,]
test<-training[-intrain,]
```
## Building the algorithm

The following two functions are used to clean and process our dataset into a form more appropriate for algorithmic analysis. 

The first function takes a dataframe, a vector of specific feature names we would like removed, and a numeric vector, resulting from the near zero variance analysis to be performed shortly. 

The second function implements a simple process for applying a preprocessing object tuned to a training on other test sets. 

```{r functions}
### Create feature elimination function
elimFeatures <- function(df, vector, nzv) {
        slim <- df[, -nzv]
        for(v in 1:length(vector)) {
                slim <- slim[, -grep(vector[v], names(slim))]
        }
        slim
}

### Create preprocessing function
prep <- function(df, preObj, main) {
        slim <- predict(preObj, df[,-grep(main, names(df))])
        slim[,main] <- df[,main]
        slim
}
```

We use the `nearZeroVar()` function found in the caret package to identify lame, useless features, void of any predictive variablity. We also specify the remaining features that we suspect will be of little use in our prediction model. Then, we run our previously defined `elimFeatures()` function, making use of both `nzv` and `metrics`. 

```{r feature elimination}
### Perform near zero variance analysis and remove useless features
nzv <- nearZeroVar(train)
metrics <- c("avg", "stddev", "var", "max", "min", "amplitude", "timestamp","window", "X", "user")

train <- elimFeatures(train, metrics, nzv)
test <- elimFeatures(test, metrics, nzv)
```

We'll use the following 52 predictors to build a prediction model on classe:
```{r}
names(train)
```

We tune our preprocessing object on the training set and use our `prep()` function to run it on out test set. 

```{r preprocessing}
### Preprocessing the training and test sets
### Create preprocessing object
preObj <- preProcess(train[,-grep("classe", names(train))],method=c("center","scale"))

### Preprocess datasets
train <- prep(train, preObj, "classe")
test <- prep(test, preObj, "classe")
```

```{r attach modFit, echo=FALSE}
attach("./objects/modFit1")
```

And finally we create our prediction model by running a randomforests prediction algorithm with cross validation on our clean and processed training set. 

```{r random forests, eval=FALSE}
### Run random forests prediction algorithm
modFit <- train(classe ~ ., data=train, method="rf", 
                trControl = trainControl(method = "cv", 
                                         number = 4, 
                                         allowParallel = TRUE, 
                                         verboseIter = TRUE))
```

## Final prediction model and out of sample error
```{r}
modFit1
modFit1$finalModel
```
As we can see, our expected out of sample error rate, using random forests, is approximately 0.74%. We can calculate this ourselves, by dividing the sum of our incorrect predictions `sum(modFit$finalModel$confusion)-sum(diag(modFit$finalModel$confusion))` by the sum of our correct predictions `sum(diag(modFit$finalModel$confusion))`:

```{r, echo=FALSE}
(sum(modFit1$finalModel$confusion)-sum(diag(modFit1$finalModel$confusion)))/sum(diag(modFit1$finalModel$confusion))
```

We apply our model out of sample, using the test subset we set aside at the beginning of our routine.

```{r predictions}
### Apply prediction model to test set
pred <- predict(modFit1, test)
test$predRight <- pred==test$classe
table(pred, test$classe)
(sum(table(pred, test$classe))-sum(diag(table(pred, test$classe))))/sum(diag(table(pred, test$classe)))
```
The error rate we obtain in our test sample is 0.77%. Not bad. 
```{r correlation analysis, echo=FALSE}
### Correlation Analysis
# MM <- abs(cor(train))
# diag(MM) <- 0
# mm <- which(MM > 0.8, arr.ind = TRUE)
```

```{r predict problem set, echo=FALSE}
### Predicting data from problem set
# testing1 <- testing
# testing1 <- elimFeatures(test1, metrics, nzv)
# testing1 <- prep(testing1, preObj, "problem_id")
# prediction1 <- predict(modFit, testing1)
# table(prediction1)
```


